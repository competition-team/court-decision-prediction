{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599454a-3185-48f2-9e0f-8a241f5f4bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from transformers import AutoTokenizer, OpenLlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import loralib as lora\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "seed_everything()\n",
    "\n",
    "\n",
    "def prompt_setting(train_first_party, train_second_party, fact):\n",
    "    instruction = \"Guess if the first party can win in a legal case. Let me know the answer with O or X.\\n\\n\"\n",
    "    prompt = instruction + 'first_party : ' + train_first_party + '\\n\\nsecond_partys : \"' + train_second_party +'\"\\n\\nfact : ' + fact + '\\n\\nanswer : '\n",
    "        \n",
    "    if len(prompt)>1000:\n",
    "        instruction = \"Guess if the first party can win in a legal case. Let me know the answer with O or X.\\n\\n\"\n",
    "        prefix = 'first_party : ' + train_first_party + '\\n\\nsecond_partys : \"' + train_second_party +'\"\\n\\nfact : '\n",
    "        fact = fact[:1000-len(instruction)-len(prefix)]\n",
    "        prompt = instruction + prefix + fact + '\\n\\nanswer : '\n",
    "        # continue\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22812d79-d455-4d58-8118-52fa26df85cb",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a6680e-2af2-4f26-9695-d98f6486a1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f96781eff54a2da48ae43a9288e3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at chainyo/alpaca-lora-7b and are newly initialized: ['model.layers.2.self_attn.v_proj.lora_B', 'model.layers.18.self_attn.v_proj.lora_B', 'model.layers.15.self_attn.v_proj.lora_A', 'model.layers.30.self_attn.q_proj.lora_B', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.lora_A', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.lora_A', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.lora_B', 'model.layers.3.self_attn.v_proj.lora_B', 'model.layers.22.self_attn.q_proj.lora_A', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.lora_B', 'model.layers.27.self_attn.q_proj.lora_A', 'model.layers.1.self_attn.q_proj.lora_B', 'model.layers.19.self_attn.q_proj.lora_A', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.lora_A', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.lora_A', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.lora_B', 'model.layers.30.self_attn.q_proj.lora_A', 'model.layers.3.self_attn.q_proj.lora_A', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.lora_B', 'model.layers.29.self_attn.v_proj.lora_B', 'model.layers.25.self_attn.v_proj.lora_B', 'model.layers.8.self_attn.v_proj.lora_A', 'model.layers.9.self_attn.q_proj.lora_B', 'model.layers.26.self_attn.q_proj.lora_B', 'model.layers.24.self_attn.v_proj.lora_A', 'model.layers.4.self_attn.v_proj.lora_A', 'model.layers.21.self_attn.q_proj.lora_A', 'model.layers.11.self_attn.v_proj.lora_A', 'model.layers.0.self_attn.q_proj.lora_B', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.lora_B', 'model.layers.28.self_attn.v_proj.lora_A', 'model.layers.9.self_attn.q_proj.lora_A', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.lora_B', 'model.layers.24.self_attn.q_proj.lora_A', 'model.layers.17.self_attn.v_proj.lora_B', 'model.layers.3.self_attn.v_proj.lora_A', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.lora_A', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.lora_A', 'model.layers.6.self_attn.q_proj.lora_A', 'model.layers.21.self_attn.q_proj.lora_B', 'model.layers.31.self_attn.q_proj.lora_A', 'model.layers.13.self_attn.v_proj.lora_B', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.lora_A', 'model.layers.10.self_attn.q_proj.lora_A', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.lora_A', 'model.layers.12.self_attn.v_proj.lora_A', 'model.layers.25.self_attn.q_proj.lora_B', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.lora_B', 'model.layers.6.self_attn.v_proj.lora_B', 'model.layers.18.self_attn.q_proj.lora_B', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.lora_A', 'model.layers.7.self_attn.q_proj.lora_B', 'model.layers.10.self_attn.q_proj.lora_B', 'model.layers.7.self_attn.v_proj.lora_B', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.lora_B', 'model.layers.24.self_attn.q_proj.lora_B', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.lora_A', 'model.layers.20.self_attn.v_proj.lora_A', 'model.layers.31.self_attn.v_proj.lora_B', 'model.layers.28.self_attn.q_proj.lora_B', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.lora_A', 'model.layers.13.self_attn.q_proj.lora_A', 'model.layers.16.self_attn.q_proj.lora_A', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.lora_B', 'model.layers.9.self_attn.v_proj.lora_B', 'model.layers.29.self_attn.v_proj.lora_A', 'model.layers.4.self_attn.q_proj.lora_A', 'model.layers.16.self_attn.v_proj.lora_B', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.23.self_attn.q_proj.lora_B', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.lora_A', 'model.layers.22.self_attn.v_proj.lora_B', 'model.layers.9.self_attn.v_proj.lora_A', 'model.layers.20.self_attn.q_proj.lora_A', 'model.layers.2.self_attn.q_proj.lora_A', 'model.layers.4.self_attn.q_proj.lora_B', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.lora_A', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.lora_B', 'model.layers.7.self_attn.v_proj.lora_A', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.lora_B', 'model.layers.29.self_attn.q_proj.lora_B', 'model.layers.5.self_attn.v_proj.lora_B', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.lora_A', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.lora_A', 'model.layers.19.self_attn.v_proj.lora_B', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.lora_A', 'model.layers.15.self_attn.q_proj.lora_A', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.lora_B', 'model.layers.22.self_attn.q_proj.lora_B', 'model.layers.13.self_attn.q_proj.lora_B', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.lora_A', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.lora_B', 'model.layers.2.self_attn.q_proj.lora_B', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.lora_A', 'model.layers.28.self_attn.v_proj.lora_B', 'model.layers.17.self_attn.q_proj.lora_B', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.lora_A', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.lora_B', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.lora_B', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.lora_B', 'model.layers.3.self_attn.q_proj.lora_B', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.lora_B', 'model.layers.23.self_attn.v_proj.lora_B', 'model.layers.23.self_attn.q_proj.lora_A', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.lora_B', 'model.layers.14.self_attn.v_proj.lora_A', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.lora_A', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.lora_A', 'model.layers.0.self_attn.q_proj.lora_A', 'model.layers.18.self_attn.q_proj.lora_A', 'model.layers.31.self_attn.q_proj.lora_B', 'model.layers.19.self_attn.q_proj.lora_B', 'model.layers.15.self_attn.q_proj.lora_B', 'model.layers.16.self_attn.v_proj.lora_A', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.lora_A', 'model.layers.12.self_attn.q_proj.lora_A', 'model.layers.12.self_attn.v_proj.lora_B', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.lora_B', 'model.layers.20.self_attn.q_proj.lora_B', 'model.layers.8.self_attn.q_proj.lora_A', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.lora_A', 'model.layers.14.self_attn.q_proj.lora_A', 'model.layers.18.self_attn.v_proj.lora_A', 'model.layers.14.self_attn.v_proj.lora_B', 'model.layers.30.self_attn.v_proj.lora_A', 'model.layers.1.self_attn.q_proj.lora_A', 'model.layers.8.self_attn.q_proj.lora_B', 'model.layers.4.self_attn.v_proj.lora_B', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.lora_A', 'model.layers.27.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "model_select = \"decapoda-research/llama-7b-hf\" #\n",
    "model_select = \"chainyo/alpaca-lora-7b\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_select)\n",
    "model = LlamaForCausalLM.from_pretrained(model_select).to(torch.bfloat16).to(\"cuda\")\n",
    "model.requires_grad = False\n",
    "lora.mark_only_lora_as_trainable(model)\n",
    "model.lm_head = nn.Linear(4096, 2)\n",
    "model.lm_head.requires_grad = True\n",
    "my_model = model.to(torch.bfloat16).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab72dbd6-42c9-4792-bad4-b1f0b02037a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.q_proj.bias False\n",
      "model.layers.0.self_attn.q_proj.lora_A True\n",
      "model.layers.0.self_attn.q_proj.lora_B True\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.bias False\n",
      "model.layers.0.self_attn.v_proj.lora_A True\n",
      "model.layers.0.self_attn.v_proj.lora_B True\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight False\n",
      "model.layers.1.self_attn.q_proj.bias False\n",
      "model.layers.1.self_attn.q_proj.lora_A True\n",
      "model.layers.1.self_attn.q_proj.lora_B True\n",
      "model.layers.1.self_attn.k_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.bias False\n",
      "model.layers.1.self_attn.v_proj.lora_A True\n",
      "model.layers.1.self_attn.v_proj.lora_B True\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.down_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight False\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.q_proj.bias False\n",
      "model.layers.2.self_attn.q_proj.lora_A True\n",
      "model.layers.2.self_attn.q_proj.lora_B True\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.bias False\n",
      "model.layers.2.self_attn.v_proj.lora_A True\n",
      "model.layers.2.self_attn.v_proj.lora_B True\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.down_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight False\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.q_proj.bias False\n",
      "model.layers.3.self_attn.q_proj.lora_A True\n",
      "model.layers.3.self_attn.q_proj.lora_B True\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.bias False\n",
      "model.layers.3.self_attn.v_proj.lora_A True\n",
      "model.layers.3.self_attn.v_proj.lora_B True\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.q_proj.bias False\n",
      "model.layers.4.self_attn.q_proj.lora_A True\n",
      "model.layers.4.self_attn.q_proj.lora_B True\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.bias False\n",
      "model.layers.4.self_attn.v_proj.lora_A True\n",
      "model.layers.4.self_attn.v_proj.lora_B True\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.q_proj.bias False\n",
      "model.layers.5.self_attn.q_proj.lora_A True\n",
      "model.layers.5.self_attn.q_proj.lora_B True\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.bias False\n",
      "model.layers.5.self_attn.v_proj.lora_A True\n",
      "model.layers.5.self_attn.v_proj.lora_B True\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.q_proj.bias False\n",
      "model.layers.6.self_attn.q_proj.lora_A True\n",
      "model.layers.6.self_attn.q_proj.lora_B True\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.bias False\n",
      "model.layers.6.self_attn.v_proj.lora_A True\n",
      "model.layers.6.self_attn.v_proj.lora_B True\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.q_proj.bias False\n",
      "model.layers.7.self_attn.q_proj.lora_A True\n",
      "model.layers.7.self_attn.q_proj.lora_B True\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.bias False\n",
      "model.layers.7.self_attn.v_proj.lora_A True\n",
      "model.layers.7.self_attn.v_proj.lora_B True\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.q_proj.bias False\n",
      "model.layers.8.self_attn.q_proj.lora_A True\n",
      "model.layers.8.self_attn.q_proj.lora_B True\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.bias False\n",
      "model.layers.8.self_attn.v_proj.lora_A True\n",
      "model.layers.8.self_attn.v_proj.lora_B True\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.q_proj.bias False\n",
      "model.layers.9.self_attn.q_proj.lora_A True\n",
      "model.layers.9.self_attn.q_proj.lora_B True\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.bias False\n",
      "model.layers.9.self_attn.v_proj.lora_A True\n",
      "model.layers.9.self_attn.v_proj.lora_B True\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.q_proj.bias False\n",
      "model.layers.10.self_attn.q_proj.lora_A True\n",
      "model.layers.10.self_attn.q_proj.lora_B True\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.bias False\n",
      "model.layers.10.self_attn.v_proj.lora_A True\n",
      "model.layers.10.self_attn.v_proj.lora_B True\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.q_proj.bias False\n",
      "model.layers.11.self_attn.q_proj.lora_A True\n",
      "model.layers.11.self_attn.q_proj.lora_B True\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.bias False\n",
      "model.layers.11.self_attn.v_proj.lora_A True\n",
      "model.layers.11.self_attn.v_proj.lora_B True\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.layers.12.self_attn.q_proj.weight False\n",
      "model.layers.12.self_attn.q_proj.bias False\n",
      "model.layers.12.self_attn.q_proj.lora_A True\n",
      "model.layers.12.self_attn.q_proj.lora_B True\n",
      "model.layers.12.self_attn.k_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.bias False\n",
      "model.layers.12.self_attn.v_proj.lora_A True\n",
      "model.layers.12.self_attn.v_proj.lora_B True\n",
      "model.layers.12.self_attn.o_proj.weight False\n",
      "model.layers.12.mlp.gate_proj.weight False\n",
      "model.layers.12.mlp.down_proj.weight False\n",
      "model.layers.12.mlp.up_proj.weight False\n",
      "model.layers.12.input_layernorm.weight False\n",
      "model.layers.12.post_attention_layernorm.weight False\n",
      "model.layers.13.self_attn.q_proj.weight False\n",
      "model.layers.13.self_attn.q_proj.bias False\n",
      "model.layers.13.self_attn.q_proj.lora_A True\n",
      "model.layers.13.self_attn.q_proj.lora_B True\n",
      "model.layers.13.self_attn.k_proj.weight False\n",
      "model.layers.13.self_attn.v_proj.weight False\n",
      "model.layers.13.self_attn.v_proj.bias False\n",
      "model.layers.13.self_attn.v_proj.lora_A True\n",
      "model.layers.13.self_attn.v_proj.lora_B True\n",
      "model.layers.13.self_attn.o_proj.weight False\n",
      "model.layers.13.mlp.gate_proj.weight False\n",
      "model.layers.13.mlp.down_proj.weight False\n",
      "model.layers.13.mlp.up_proj.weight False\n",
      "model.layers.13.input_layernorm.weight False\n",
      "model.layers.13.post_attention_layernorm.weight False\n",
      "model.layers.14.self_attn.q_proj.weight False\n",
      "model.layers.14.self_attn.q_proj.bias False\n",
      "model.layers.14.self_attn.q_proj.lora_A True\n",
      "model.layers.14.self_attn.q_proj.lora_B True\n",
      "model.layers.14.self_attn.k_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.bias False\n",
      "model.layers.14.self_attn.v_proj.lora_A True\n",
      "model.layers.14.self_attn.v_proj.lora_B True\n",
      "model.layers.14.self_attn.o_proj.weight False\n",
      "model.layers.14.mlp.gate_proj.weight False\n",
      "model.layers.14.mlp.down_proj.weight False\n",
      "model.layers.14.mlp.up_proj.weight False\n",
      "model.layers.14.input_layernorm.weight False\n",
      "model.layers.14.post_attention_layernorm.weight False\n",
      "model.layers.15.self_attn.q_proj.weight False\n",
      "model.layers.15.self_attn.q_proj.bias False\n",
      "model.layers.15.self_attn.q_proj.lora_A True\n",
      "model.layers.15.self_attn.q_proj.lora_B True\n",
      "model.layers.15.self_attn.k_proj.weight False\n",
      "model.layers.15.self_attn.v_proj.weight False\n",
      "model.layers.15.self_attn.v_proj.bias False\n",
      "model.layers.15.self_attn.v_proj.lora_A True\n",
      "model.layers.15.self_attn.v_proj.lora_B True\n",
      "model.layers.15.self_attn.o_proj.weight False\n",
      "model.layers.15.mlp.gate_proj.weight False\n",
      "model.layers.15.mlp.down_proj.weight False\n",
      "model.layers.15.mlp.up_proj.weight False\n",
      "model.layers.15.input_layernorm.weight False\n",
      "model.layers.15.post_attention_layernorm.weight False\n",
      "model.layers.16.self_attn.q_proj.weight False\n",
      "model.layers.16.self_attn.q_proj.bias False\n",
      "model.layers.16.self_attn.q_proj.lora_A True\n",
      "model.layers.16.self_attn.q_proj.lora_B True\n",
      "model.layers.16.self_attn.k_proj.weight False\n",
      "model.layers.16.self_attn.v_proj.weight False\n",
      "model.layers.16.self_attn.v_proj.bias False\n",
      "model.layers.16.self_attn.v_proj.lora_A True\n",
      "model.layers.16.self_attn.v_proj.lora_B True\n",
      "model.layers.16.self_attn.o_proj.weight False\n",
      "model.layers.16.mlp.gate_proj.weight False\n",
      "model.layers.16.mlp.down_proj.weight False\n",
      "model.layers.16.mlp.up_proj.weight False\n",
      "model.layers.16.input_layernorm.weight False\n",
      "model.layers.16.post_attention_layernorm.weight False\n",
      "model.layers.17.self_attn.q_proj.weight False\n",
      "model.layers.17.self_attn.q_proj.bias False\n",
      "model.layers.17.self_attn.q_proj.lora_A True\n",
      "model.layers.17.self_attn.q_proj.lora_B True\n",
      "model.layers.17.self_attn.k_proj.weight False\n",
      "model.layers.17.self_attn.v_proj.weight False\n",
      "model.layers.17.self_attn.v_proj.bias False\n",
      "model.layers.17.self_attn.v_proj.lora_A True\n",
      "model.layers.17.self_attn.v_proj.lora_B True\n",
      "model.layers.17.self_attn.o_proj.weight False\n",
      "model.layers.17.mlp.gate_proj.weight False\n",
      "model.layers.17.mlp.down_proj.weight False\n",
      "model.layers.17.mlp.up_proj.weight False\n",
      "model.layers.17.input_layernorm.weight False\n",
      "model.layers.17.post_attention_layernorm.weight False\n",
      "model.layers.18.self_attn.q_proj.weight False\n",
      "model.layers.18.self_attn.q_proj.bias False\n",
      "model.layers.18.self_attn.q_proj.lora_A True\n",
      "model.layers.18.self_attn.q_proj.lora_B True\n",
      "model.layers.18.self_attn.k_proj.weight False\n",
      "model.layers.18.self_attn.v_proj.weight False\n",
      "model.layers.18.self_attn.v_proj.bias False\n",
      "model.layers.18.self_attn.v_proj.lora_A True\n",
      "model.layers.18.self_attn.v_proj.lora_B True\n",
      "model.layers.18.self_attn.o_proj.weight False\n",
      "model.layers.18.mlp.gate_proj.weight False\n",
      "model.layers.18.mlp.down_proj.weight False\n",
      "model.layers.18.mlp.up_proj.weight False\n",
      "model.layers.18.input_layernorm.weight False\n",
      "model.layers.18.post_attention_layernorm.weight False\n",
      "model.layers.19.self_attn.q_proj.weight False\n",
      "model.layers.19.self_attn.q_proj.bias False\n",
      "model.layers.19.self_attn.q_proj.lora_A True\n",
      "model.layers.19.self_attn.q_proj.lora_B True\n",
      "model.layers.19.self_attn.k_proj.weight False\n",
      "model.layers.19.self_attn.v_proj.weight False\n",
      "model.layers.19.self_attn.v_proj.bias False\n",
      "model.layers.19.self_attn.v_proj.lora_A True\n",
      "model.layers.19.self_attn.v_proj.lora_B True\n",
      "model.layers.19.self_attn.o_proj.weight False\n",
      "model.layers.19.mlp.gate_proj.weight False\n",
      "model.layers.19.mlp.down_proj.weight False\n",
      "model.layers.19.mlp.up_proj.weight False\n",
      "model.layers.19.input_layernorm.weight False\n",
      "model.layers.19.post_attention_layernorm.weight False\n",
      "model.layers.20.self_attn.q_proj.weight False\n",
      "model.layers.20.self_attn.q_proj.bias False\n",
      "model.layers.20.self_attn.q_proj.lora_A True\n",
      "model.layers.20.self_attn.q_proj.lora_B True\n",
      "model.layers.20.self_attn.k_proj.weight False\n",
      "model.layers.20.self_attn.v_proj.weight False\n",
      "model.layers.20.self_attn.v_proj.bias False\n",
      "model.layers.20.self_attn.v_proj.lora_A True\n",
      "model.layers.20.self_attn.v_proj.lora_B True\n",
      "model.layers.20.self_attn.o_proj.weight False\n",
      "model.layers.20.mlp.gate_proj.weight False\n",
      "model.layers.20.mlp.down_proj.weight False\n",
      "model.layers.20.mlp.up_proj.weight False\n",
      "model.layers.20.input_layernorm.weight False\n",
      "model.layers.20.post_attention_layernorm.weight False\n",
      "model.layers.21.self_attn.q_proj.weight False\n",
      "model.layers.21.self_attn.q_proj.bias False\n",
      "model.layers.21.self_attn.q_proj.lora_A True\n",
      "model.layers.21.self_attn.q_proj.lora_B True\n",
      "model.layers.21.self_attn.k_proj.weight False\n",
      "model.layers.21.self_attn.v_proj.weight False\n",
      "model.layers.21.self_attn.v_proj.bias False\n",
      "model.layers.21.self_attn.v_proj.lora_A True\n",
      "model.layers.21.self_attn.v_proj.lora_B True\n",
      "model.layers.21.self_attn.o_proj.weight False\n",
      "model.layers.21.mlp.gate_proj.weight False\n",
      "model.layers.21.mlp.down_proj.weight False\n",
      "model.layers.21.mlp.up_proj.weight False\n",
      "model.layers.21.input_layernorm.weight False\n",
      "model.layers.21.post_attention_layernorm.weight False\n",
      "model.layers.22.self_attn.q_proj.weight False\n",
      "model.layers.22.self_attn.q_proj.bias False\n",
      "model.layers.22.self_attn.q_proj.lora_A True\n",
      "model.layers.22.self_attn.q_proj.lora_B True\n",
      "model.layers.22.self_attn.k_proj.weight False\n",
      "model.layers.22.self_attn.v_proj.weight False\n",
      "model.layers.22.self_attn.v_proj.bias False\n",
      "model.layers.22.self_attn.v_proj.lora_A True\n",
      "model.layers.22.self_attn.v_proj.lora_B True\n",
      "model.layers.22.self_attn.o_proj.weight False\n",
      "model.layers.22.mlp.gate_proj.weight False\n",
      "model.layers.22.mlp.down_proj.weight False\n",
      "model.layers.22.mlp.up_proj.weight False\n",
      "model.layers.22.input_layernorm.weight False\n",
      "model.layers.22.post_attention_layernorm.weight False\n",
      "model.layers.23.self_attn.q_proj.weight False\n",
      "model.layers.23.self_attn.q_proj.bias False\n",
      "model.layers.23.self_attn.q_proj.lora_A True\n",
      "model.layers.23.self_attn.q_proj.lora_B True\n",
      "model.layers.23.self_attn.k_proj.weight False\n",
      "model.layers.23.self_attn.v_proj.weight False\n",
      "model.layers.23.self_attn.v_proj.bias False\n",
      "model.layers.23.self_attn.v_proj.lora_A True\n",
      "model.layers.23.self_attn.v_proj.lora_B True\n",
      "model.layers.23.self_attn.o_proj.weight False\n",
      "model.layers.23.mlp.gate_proj.weight False\n",
      "model.layers.23.mlp.down_proj.weight False\n",
      "model.layers.23.mlp.up_proj.weight False\n",
      "model.layers.23.input_layernorm.weight False\n",
      "model.layers.23.post_attention_layernorm.weight False\n",
      "model.layers.24.self_attn.q_proj.weight False\n",
      "model.layers.24.self_attn.q_proj.bias False\n",
      "model.layers.24.self_attn.q_proj.lora_A True\n",
      "model.layers.24.self_attn.q_proj.lora_B True\n",
      "model.layers.24.self_attn.k_proj.weight False\n",
      "model.layers.24.self_attn.v_proj.weight False\n",
      "model.layers.24.self_attn.v_proj.bias False\n",
      "model.layers.24.self_attn.v_proj.lora_A True\n",
      "model.layers.24.self_attn.v_proj.lora_B True\n",
      "model.layers.24.self_attn.o_proj.weight False\n",
      "model.layers.24.mlp.gate_proj.weight False\n",
      "model.layers.24.mlp.down_proj.weight False\n",
      "model.layers.24.mlp.up_proj.weight False\n",
      "model.layers.24.input_layernorm.weight False\n",
      "model.layers.24.post_attention_layernorm.weight False\n",
      "model.layers.25.self_attn.q_proj.weight False\n",
      "model.layers.25.self_attn.q_proj.bias False\n",
      "model.layers.25.self_attn.q_proj.lora_A True\n",
      "model.layers.25.self_attn.q_proj.lora_B True\n",
      "model.layers.25.self_attn.k_proj.weight False\n",
      "model.layers.25.self_attn.v_proj.weight False\n",
      "model.layers.25.self_attn.v_proj.bias False\n",
      "model.layers.25.self_attn.v_proj.lora_A True\n",
      "model.layers.25.self_attn.v_proj.lora_B True\n",
      "model.layers.25.self_attn.o_proj.weight False\n",
      "model.layers.25.mlp.gate_proj.weight False\n",
      "model.layers.25.mlp.down_proj.weight False\n",
      "model.layers.25.mlp.up_proj.weight False\n",
      "model.layers.25.input_layernorm.weight False\n",
      "model.layers.25.post_attention_layernorm.weight False\n",
      "model.layers.26.self_attn.q_proj.weight False\n",
      "model.layers.26.self_attn.q_proj.bias False\n",
      "model.layers.26.self_attn.q_proj.lora_A True\n",
      "model.layers.26.self_attn.q_proj.lora_B True\n",
      "model.layers.26.self_attn.k_proj.weight False\n",
      "model.layers.26.self_attn.v_proj.weight False\n",
      "model.layers.26.self_attn.v_proj.bias False\n",
      "model.layers.26.self_attn.v_proj.lora_A True\n",
      "model.layers.26.self_attn.v_proj.lora_B True\n",
      "model.layers.26.self_attn.o_proj.weight False\n",
      "model.layers.26.mlp.gate_proj.weight False\n",
      "model.layers.26.mlp.down_proj.weight False\n",
      "model.layers.26.mlp.up_proj.weight False\n",
      "model.layers.26.input_layernorm.weight False\n",
      "model.layers.26.post_attention_layernorm.weight False\n",
      "model.layers.27.self_attn.q_proj.weight False\n",
      "model.layers.27.self_attn.q_proj.bias False\n",
      "model.layers.27.self_attn.q_proj.lora_A True\n",
      "model.layers.27.self_attn.q_proj.lora_B True\n",
      "model.layers.27.self_attn.k_proj.weight False\n",
      "model.layers.27.self_attn.v_proj.weight False\n",
      "model.layers.27.self_attn.v_proj.bias False\n",
      "model.layers.27.self_attn.v_proj.lora_A True\n",
      "model.layers.27.self_attn.v_proj.lora_B True\n",
      "model.layers.27.self_attn.o_proj.weight False\n",
      "model.layers.27.mlp.gate_proj.weight False\n",
      "model.layers.27.mlp.down_proj.weight False\n",
      "model.layers.27.mlp.up_proj.weight False\n",
      "model.layers.27.input_layernorm.weight False\n",
      "model.layers.27.post_attention_layernorm.weight False\n",
      "model.layers.28.self_attn.q_proj.weight False\n",
      "model.layers.28.self_attn.q_proj.bias False\n",
      "model.layers.28.self_attn.q_proj.lora_A True\n",
      "model.layers.28.self_attn.q_proj.lora_B True\n",
      "model.layers.28.self_attn.k_proj.weight False\n",
      "model.layers.28.self_attn.v_proj.weight False\n",
      "model.layers.28.self_attn.v_proj.bias False\n",
      "model.layers.28.self_attn.v_proj.lora_A True\n",
      "model.layers.28.self_attn.v_proj.lora_B True\n",
      "model.layers.28.self_attn.o_proj.weight False\n",
      "model.layers.28.mlp.gate_proj.weight False\n",
      "model.layers.28.mlp.down_proj.weight False\n",
      "model.layers.28.mlp.up_proj.weight False\n",
      "model.layers.28.input_layernorm.weight False\n",
      "model.layers.28.post_attention_layernorm.weight False\n",
      "model.layers.29.self_attn.q_proj.weight False\n",
      "model.layers.29.self_attn.q_proj.bias False\n",
      "model.layers.29.self_attn.q_proj.lora_A True\n",
      "model.layers.29.self_attn.q_proj.lora_B True\n",
      "model.layers.29.self_attn.k_proj.weight False\n",
      "model.layers.29.self_attn.v_proj.weight False\n",
      "model.layers.29.self_attn.v_proj.bias False\n",
      "model.layers.29.self_attn.v_proj.lora_A True\n",
      "model.layers.29.self_attn.v_proj.lora_B True\n",
      "model.layers.29.self_attn.o_proj.weight False\n",
      "model.layers.29.mlp.gate_proj.weight False\n",
      "model.layers.29.mlp.down_proj.weight False\n",
      "model.layers.29.mlp.up_proj.weight False\n",
      "model.layers.29.input_layernorm.weight False\n",
      "model.layers.29.post_attention_layernorm.weight False\n",
      "model.layers.30.self_attn.q_proj.weight False\n",
      "model.layers.30.self_attn.q_proj.bias False\n",
      "model.layers.30.self_attn.q_proj.lora_A True\n",
      "model.layers.30.self_attn.q_proj.lora_B True\n",
      "model.layers.30.self_attn.k_proj.weight False\n",
      "model.layers.30.self_attn.v_proj.weight False\n",
      "model.layers.30.self_attn.v_proj.bias False\n",
      "model.layers.30.self_attn.v_proj.lora_A True\n",
      "model.layers.30.self_attn.v_proj.lora_B True\n",
      "model.layers.30.self_attn.o_proj.weight False\n",
      "model.layers.30.mlp.gate_proj.weight False\n",
      "model.layers.30.mlp.down_proj.weight False\n",
      "model.layers.30.mlp.up_proj.weight False\n",
      "model.layers.30.input_layernorm.weight False\n",
      "model.layers.30.post_attention_layernorm.weight False\n",
      "model.layers.31.self_attn.q_proj.weight False\n",
      "model.layers.31.self_attn.q_proj.bias False\n",
      "model.layers.31.self_attn.q_proj.lora_A True\n",
      "model.layers.31.self_attn.q_proj.lora_B True\n",
      "model.layers.31.self_attn.k_proj.weight False\n",
      "model.layers.31.self_attn.v_proj.weight False\n",
      "model.layers.31.self_attn.v_proj.bias False\n",
      "model.layers.31.self_attn.v_proj.lora_A True\n",
      "model.layers.31.self_attn.v_proj.lora_B True\n",
      "model.layers.31.self_attn.o_proj.weight False\n",
      "model.layers.31.mlp.gate_proj.weight False\n",
      "model.layers.31.mlp.down_proj.weight False\n",
      "model.layers.31.mlp.up_proj.weight False\n",
      "model.layers.31.input_layernorm.weight False\n",
      "model.layers.31.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n",
      "lm_head.weight True\n",
      "lm_head.bias True\n"
     ]
    }
   ],
   "source": [
    "# for name, param in my_model.named_parameters():\n",
    "#     if 'lm_head' in name:\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "    \n",
    "for name, param in my_model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70cda2b4-7d90-4de9-9998-9d532b49cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class classifier(nn.Module):\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.lm = model\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(32000, 2),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.lm(x)\n",
    "#         logits = self.linear_relu_stack(x.logits[:,-1,:])\n",
    "#         return logits\n",
    "# my_model = classifier(model)\n",
    "# my_model.to(torch.bfloat16).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdec37fd-664d-4e15-a6ff-f4e8dbe1c568",
   "metadata": {},
   "source": [
    "# data setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcbc15f-5dde-4bec-aba2-bff9ed1aeff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_partys = train['first_party'].tolist()\n",
    "second_partys = train['second_party'].tolist()\n",
    "facts = train['facts'].tolist()\n",
    "first_party_winners = train['first_party_winner'].tolist()\n",
    "index = int(len(first_partys)*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ae0c7a-c3dd-434b-adf0-6c7fa5f29360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first_partys = first_partys[:index]\n",
    "train_second_partys = second_partys[:index]\n",
    "train_facts = facts[:index]\n",
    "train_first_party_winners = first_party_winners[:index]\n",
    "\n",
    "\n",
    "train_temp = list(zip(train_first_partys, train_second_partys, train_facts, train_first_party_winners))\n",
    "\n",
    "random.shuffle(train_temp)\n",
    "\n",
    "train_first_partys, train_second_partys, train_facts, train_first_party_winners = zip(*train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a61f7a-9ce5-4541-9a05-4ff7c205338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "85\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "test_first_partys = first_partys[index:]\n",
    "test_second_partys = second_partys[index:]\n",
    "test_facts = facts[index:]\n",
    "test_first_party_winners = first_party_winners[index:]\n",
    "\n",
    "\n",
    "test_facts = np.array(test_facts)\n",
    "test_first_partys = np.array(test_first_partys)\n",
    "test_second_partys = np.array(test_second_partys)\n",
    "test_first_party_winners = np.array(test_first_party_winners)\n",
    "\n",
    "num_zero = np.sum(test_first_party_winners==0)\n",
    "zero_test_facts = test_facts[test_first_party_winners==0]\n",
    "zero_test_first_partys = test_first_partys[test_first_party_winners==0]\n",
    "zero_test_second_partys= test_second_partys[test_first_party_winners==0]\n",
    "zero_test_first_party_winners = test_first_party_winners[test_first_party_winners==0]\n",
    "\n",
    "\n",
    "one_test_facts = test_facts[test_first_party_winners==1]\n",
    "one_test_first_partys = test_first_partys[test_first_party_winners==1]\n",
    "one_test_second_partys= test_second_partys[test_first_party_winners==1]\n",
    "one_test_first_party_winners = test_first_party_winners[test_first_party_winners==1]\n",
    "\n",
    "one_test_facts = one_test_facts[:num_zero]\n",
    "one_test_first_partys = one_test_first_partys[:num_zero]\n",
    "one_test_second_partys = one_test_second_partys[:num_zero]\n",
    "one_test_first_party_winners = one_test_first_party_winners[:num_zero]\n",
    "\n",
    "\n",
    "print(len(zero_test_first_party_winners))\n",
    "print(len(one_test_first_party_winners))\n",
    "\n",
    "test_first_partys = zero_test_first_partys.tolist() + one_test_first_partys.tolist()\n",
    "test_second_partys = zero_test_second_partys.tolist() + one_test_second_partys.tolist()\n",
    "test_facts = zero_test_facts.tolist() + one_test_facts.tolist()\n",
    "test_first_party_winners = zero_test_first_party_winners.tolist() + one_test_first_party_winners.tolist()\n",
    "\n",
    "print(len(test_first_party_winners))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61a83d-bca9-4412-a915-93582558bf3a",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18d424-6e74-4059-80bc-00e9925ed300",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "avgtrain = []\n",
    "avgtest = []\n",
    "bestloss = 0\n",
    "maxprompt = 0\n",
    "for epoch in range(epochs):\n",
    "    trainlist = []\n",
    "    testlist = []\n",
    "    \n",
    "    random.shuffle(train_temp)\n",
    "    train_first_partys, train_second_partys, train_facts, train_first_party_winners = zip(*train_temp)\n",
    "    count = 0\n",
    "    loss = None\n",
    "    for train_first_party, train_second_party, fact, first_party_winner in zip(train_first_partys, train_second_partys, train_facts, train_first_party_winners):\n",
    "        prompt = prompt_setting(train_first_party, train_second_party, fact)\n",
    "        \n",
    "        if first_party_winner==1:\n",
    "            labels = torch.tensor([1]).to('cuda') \n",
    "        else:\n",
    "            labels = torch.tensor([0]).to('cuda')\n",
    "            \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input1 = inputs.input_ids.to('cuda')\n",
    "        ouputs = my_model(input1)\n",
    "        \n",
    "        loss_temp = nn.CrossEntropyLoss()\n",
    "        if (count+1)%4==0:\n",
    "            loss += loss_temp(ouputs.logits[:,-1,:], labels)\n",
    "            loss /= 4           \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # trainlist.append(loss.item())\n",
    "            del loss\n",
    "            loss = None\n",
    "        else:\n",
    "            if loss==None:\n",
    "                loss = loss_temp(ouputs.logits[:,-1,:], labels)\n",
    "            else:\n",
    "                loss += loss_temp(ouputs.logits[:,-1,:], labels)\n",
    "        \n",
    "        del ouputs\n",
    "        del inputs\n",
    "        del input1\n",
    "        count += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for train_first_party, train_second_party, fact, first_party_winner in zip(test_first_partys, test_second_partys, test_facts, test_first_party_winners):\n",
    "            prompt = prompt_setting(train_first_party, train_second_party, fact)\n",
    "\n",
    "            if first_party_winner==1:\n",
    "                labels = torch.tensor([1]).to('cuda') \n",
    "            else:\n",
    "                labels = torch.tensor([0]).to('cuda')    \n",
    "            \n",
    "            loss_temp = nn.CrossEntropyLoss()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            input1 = inputs.input_ids.to('cuda')\n",
    "            ouputs = my_model(input1)\n",
    "            loss = loss_temp(ouputs.logits[:,-1,:], labels)\n",
    "            \n",
    "            if first_party_winner==1:\n",
    "                if ouputs.logits[:,-1,0]>ouputs.logits[:,-1,1]:\n",
    "                    testlist.append(0)\n",
    "                else:\n",
    "                    testlist.append(1)\n",
    "            else:\n",
    "                if ouputs.logits[:,-1,0]>ouputs.logits[:,-1,1]:\n",
    "                    testlist.append(1)\n",
    "                else:\n",
    "                    testlist.append(0)\n",
    "                    \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        for train_first_party, train_second_party, fact, first_party_winner in zip(train_first_partys, train_second_partys, train_facts, train_first_party_winners):\n",
    "            prompt = prompt_setting(train_first_party, train_second_party, fact)\n",
    "            if first_party_winner==1:\n",
    "                labels = torch.tensor([1]).to('cuda') \n",
    "            else:\n",
    "                labels = torch.tensor([0]).to('cuda')    \n",
    "            \n",
    "            loss_temp = nn.CrossEntropyLoss()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            input1 = inputs.input_ids.to('cuda')\n",
    "            ouputs = my_model(input1)\n",
    "            loss = loss_temp(ouputs.logits[:,-1,:], labels)\n",
    "            \n",
    "            if first_party_winner==1:\n",
    "                if ouputs.logits[:,-1,0]>ouputs.logits[:,-1,1]:\n",
    "                    trainlist.append(0)\n",
    "                else:\n",
    "                    trainlist.append(1)\n",
    "            else:\n",
    "                if ouputs.logits[:,-1,0]>ouputs.logits[:,-1,1]:\n",
    "                    trainlist.append(1)\n",
    "                else:\n",
    "                    trainlist.append(0)\n",
    "                    \n",
    "            torch.cuda.empty_cache()\n",
    "    avgtest.append(np.mean(testlist))\n",
    "    avgtrain.append(np.mean(trainlist))\n",
    "    if np.mean(testlist)>bestloss:\n",
    "        bestloss = np.mean(testlist)\n",
    "        torch.save(lora.lora_state_dict(model), \"temp.pt\")\n",
    "        torch.save(model.lm_head.state_dict(), \"temp_h.pt\")\n",
    "    trainlist = []\n",
    "    testlist = []\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.plot(avgtest, color='r')\n",
    "    plt.plot(avgtrain, color='b')\n",
    "    plt.savefig('loss.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c280b5a-d0a1-4e7f-8f75-80d4292c81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ouputs = model(input1, labels = input2)\n",
    "print(ouputs.loss.item())\n",
    "optimizer.zero_grad()\n",
    "ouputs.loss.requires_grad_(True)\n",
    "ouputs.loss.backward()\n",
    "optimizer.step()\n",
    "print(ouputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10895f20-b475-4da4-b50c-699769347043",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39748670-ef3c-49b6-bb7f-770a8094b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95c395-b586-4557-aa67-312ca454476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1d0c2-92a7-4672-b7fd-5f9608aa567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "o 438, x ; 1060\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
